{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "405221e5",
   "metadata": {},
   "source": [
    "# \"Code-Driven Object Classification Showdown\"\n",
    "\n",
    "Objective:\n",
    "To use some basic coding skills and understanding of object classification and detection model in a real-world scenario.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a113bdf3",
   "metadata": {},
   "source": [
    "# Install libraries \n",
    "\n",
    "Before you can use a library in your code, you need to install it on your system.\n",
    "Installation is the process of downloading and configuring the library files on your computer.\n",
    "This is typically done once per environment (though it may need to be repeated if you need to update to a newer version of the library).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a87067c",
   "metadata": {},
   "source": [
    "# You may not need to install a library in the following scenatios:\n",
    "\n",
    "1.Standard Library Modules:\n",
    "Python comes with a set of standard libraries that are included with the installation of Python. For these libraries, you don't need to perform any additional installation. Examples include math, datetime, and json.\n",
    "\n",
    "2.Pre-installed Libraries:\n",
    "If you are working in an environment where the library has already been installed, either by yourself or someone else, you don't need to install it again. This is common in shared environments, company setups, or pre-configured environments like Anaconda.\n",
    "\n",
    "3.Built-in Modules:\n",
    "Some frameworks or larger libraries come with other libraries bundled in. In these cases, you don't need to install the bundled libraries separately.\n",
    "\n",
    "4.Using Online IDEs:\n",
    "When you are using online Integrated Development Environments (IDEs) like Google Colab or Jupyter Notebooks hosted on a cloud service, many common libraries are pre-installed, and you don't need to install them unless you need a library that is not included.\n",
    "\n",
    "5.Custom Environments:\n",
    "In custom environments like Docker containers or virtual environments that have been set up with a specific set of libraries, you may not need to install anything if the libraries you need are already included.\n",
    "\n",
    "6.Cloud Services with Pre-configured Environments:\n",
    "Some cloud services offer pre-configured environments with a wide array of libraries pre-installed. In such cases, you might not need to install anything.\n",
    "\n",
    "7.Notebooks with Binder:\n",
    "If you are using a Jupyter Notebook with Binder, the libraries might be specified in a configuration file and installed automatically when the Binder environment is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d26c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Here, we import all necessary libraries and modules.\n",
    "# TensorFlow is used for loading the CIFAR-10 dataset, NumPy for numerical operations, \n",
    "# Matplotlib for plotting, and scikit-learn for data splitting, model training, evaluation, and fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c81e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "# Load CIFAR-10 dataset\n",
    "# use TensorFlow's Keras API to load the CIFAR-10 dataset. \n",
    "# Split  this dataset into training and testing sets, each with its respective images and labels.\n",
    " #-- your code here--\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#x_train, y_train, x_test, y_test = tf.keras.datasets.cifar10.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e66aea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the labels\n",
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()\n",
    "\n",
    "# Why do we flatten the  labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3190c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train.reshape(x_train.shape[0], -1))\n",
    "x_test = scaler.transform(x_test.reshape(x_test.shape[0], -1))\n",
    "\n",
    "#We scale the data to have a mean of 0 and a variance of 1, which often leads to better performance with k-NN. \n",
    "#We also reshape the images from 3D arrays to 2D arrays as scikit-learn expects 2D input.\n",
    "\n",
    "# Discuss the importance and how to calculate  Mean and Variance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16be3bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display images from the dataset\n",
    "def display_images(images, labels):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(10, 5),\n",
    "                             subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                             gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(images[i].reshape(32, 32, 3).astype(np.uint8))\n",
    "        ax.set_title(labels[i])\n",
    "    plt.show()\n",
    "    \n",
    "#This function is used to display images along with their labels. It's helpful for visual verification of the data.\n",
    "#This function takes in a set of images and labels, and displays the images in a single row using Matplotlib. \n",
    "#Each image is reshaped back to its original dimensions (32x32x3) and\n",
    "#the pixel values are cast to unsigned 8-bit integers (which is the expected format for displaying images using Matplotlib).\n",
    "#This code sets up a good foundation for the subsequent steps in the assignment,\n",
    "#such as filtering out specific categories of images, training a k-NN classifier, and evaluating its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875ce8b9",
   "metadata": {},
   "source": [
    "**The CIFAR-10 dataset is composed of 60,000 32x32 color images, categorized into 10 different classes.**\n",
    "Each class has 6,000 images. The categories are as follows:\n",
    "\n",
    "Airplane, \n",
    "Automobile, \n",
    "Bird, \n",
    "Cat,  \n",
    "Deer, \n",
    "Dog, \n",
    "Frog, \n",
    "Horse, \n",
    "Ship, \n",
    "Truck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078baae5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Filter out images from your assigned category (e.g., 'dog')\n",
    "#    Hint: Use np.where to find indices of 'dog' labels in y_train\n",
    "\n",
    "# --- Your code here ---\n",
    "'category'_indices = np.where(y_train == 0)[0]  # '0' corresponds to 'airplane' in CIFAR-10\n",
    "'category'_images = x_train['category_indices]\n",
    "'category'_labels = y_train['category_indices]\n",
    "\n",
    "# --- Your code here ---\n",
    "# Here, we filter out the images of airplanes from the training data for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e317e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Display some images from your assigned category using the display_images function\n",
    "\n",
    "#-- Your Code Here --\n",
    "\n",
    "# Task 2: Display some images from the 'airplane' category\n",
    "\n",
    "\n",
    "# --- Your code here ---\n",
    "\n",
    "#We call the display function to visualize some  images of the category you picked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b162a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Train a k-NN classifier and evaluate it\n",
    "#    Hint: You can use KNeighborsClassifier from scikit-learn\n",
    "\n",
    "\n",
    "k_values = list(range(1, 11))\n",
    "cv_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, x_train, y_train, cv=5, scoring='accuracy')\n",
    "    cv_scores.append(np.mean(scores))\n",
    "\n",
    "\n",
    "# -- Your Code Here--\n",
    "#We train k-NN classifiers for different values of k (number of neighbors)\n",
    "#and evaluate them using 5-fold cross-validation to estimate the misclassification error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2701592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bias-variance\n",
    "mse = [1 - x for x in cv_scores]\n",
    "plt.plot(k_values, mse)\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Misclassification Error')\n",
    "plt.title('Bias-Variance Plot')\n",
    "plt.show()\n",
    "\n",
    "#We plot the misclassification error against k to visualize the bias-variance trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d9a08d",
   "metadata": {},
   "source": [
    " # How Bias and Variance Come into Play'\n",
    "\n",
    "**High Bias**: If your trained model consistently makes predictions that are off the mark for different datasets (like consistently throwing darts to the left of the bullseye), it has a high bias. It's probably oversimplifying the problem.\n",
    "**ML Solution:** Use a more complex model or incorporate more features.\n",
    "\n",
    "**High Variance:** If your model makes predictions that vary a lot for slight changes in the training data (like your darts scattering all over the board based on tiny differences in your stance or grip), it has high variance. It's probably overfitting to the training data.\n",
    "**ML Solution:** Collect more data, use regularization, or choose a simpler model.\n",
    "\n",
    "**The Role of Bias and Variance in ML:**\n",
    "Model Evaluation: Understanding bias and variance helps in evaluating a model's performance. If a model has high error on the training set, it's likely due to high bias. If it performs well on the training set but poorly on the validation set, it's likely due to high variance.\n",
    "\n",
    "**Model Improvement:** By diagnosing whether a model suffers from high bias, high variance, or both, you can take steps to improve it.\n",
    "\n",
    "**The Trade-off:** In ML, there's often a trade-off. Reducing bias might increase variance and vice versa. The goal is to find the sweet spot where the model has a balanced bias-variance, leading to the most accurate predictions on unseen data.\n",
    "\n",
    "Bias and variance are fundamental concepts in ML, influencing model performance, evaluation, and improvement strategies.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9414f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning the model using GridSearchCV\n",
    "param_grid = {'n_neighbors': k_values}\n",
    "knn = KNeighborsClassifier()\n",
    "grid = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')\n",
    "grid.fit(x_train, y_train)\n",
    "\n",
    "# We use GridSearchCV to perform a grid search over different values of k, \n",
    "#finding the optimal k value that minimizes the misclassification error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df46fe2a",
   "metadata": {},
   "source": [
    "# GridSearchCV\n",
    " It is a library function provided in Scikit-Learn in Python, and it stands for Grid Search Cross Validation. It's used to find the best set of hyperparameters for a machine learning model which in turn helps to improve the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74253669",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. (Optional) Try different values of k and observe how the classification accuracy changes\n",
    "\n",
    "\n",
    "\n",
    "for k in range(1, 10):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train, y_train)\n",
    "    y_pred = knn.predict(x_test)\n",
    "    print(f'k={k}, Accuracy: {accuracy_score(y_test, y_pred)*100:.2f}%')\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83583af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.Display the optimal value of k and evaluate the model\n",
    "# Hint: You can use the accuracy_score function from scikit-learn\n",
    "\n",
    "best_k = grid.best_params_['n_neighbors']\n",
    "best_knn = grid.best_estimator_\n",
    "y_pred = best_knn.predict(x_test)\n",
    "print(f'Optimal k: {best_k}')\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)*100:.2f}%')\n",
    "\n",
    "\n",
    "##We display the optimal k value and evaluate the best k-NN model on the test data to check its performance.\n",
    "#Each step in the code is necessary to ensure proper data preparation, model training, evaluation, and fine-tuning, \n",
    "#which are crucial aspects of building and understanding machine learning models\n",
    "\n",
    "# Set a flag to indicate that the code cell has completed execution\n",
    "code_completed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4ec1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Replace 'True' with your condition. \n",
    "# For example, if you have a variable called 'result' and you want to display the Markdown\n",
    "# only if 'result' is greater than 10, replace 'True' with 'result > 10'.\n",
    "condition = True\n",
    "\n",
    "if 'code_completed' in globals() and code_completed:\n",
    "    display(Markdown(\"\"\"\n",
    "    ## Why do you think of this model accuracy? How can it be improved?\n",
    "\n",
    "    How to improve Model's accuracy? Here are some suggestions on how to improve the model\n",
    "\n",
    "    In this assignment, there could be various reasons for a low accuracy score. Here are some potential factors and suggestions on how to improve the model performance:\n",
    "\n",
    "    - **Network Architecture**: The architecture of the neural network can significantly affect the performance. A more complex architecture with additional layers might capture the features of the data better. You could experiment with adding more convolutional layers, pooling layers, or fully connected layers to the network.\n",
    "\n",
    "    - **Hyperparameters**: Tuning hyperparameters such as the learning rate, batch size, and the number of epochs can also help in improving the model's performance. You can use techniques like GridSearchCV or RandomizedSearchCV to find the optimal hyperparameters.\n",
    "\n",
    "    - **Data Augmentation**: Augmenting the training data by applying random transformations (e.g., rotation, scaling, cropping, flipping) can help the model learn more robust features and improve its generalization ability.\n",
    "\n",
    "    - **Normalization and Preprocessing**: Ensuring that the input data is normalized and preprocessed correctly is crucial. Normalizing the pixel values of the images to a range between 0 and 1 can help in speeding up the training and improving the convergence of the model.\n",
    "\n",
    "    - **Advanced Optimizers**: Experimenting with different optimization algorithms such as Adam, RMSprop, or Adagrad, might lead to better training convergence and improved accuracy.\n",
    "\n",
    "    - **Regularization**: Implementing regularization techniques such as dropout or L2 regularization can help in reducing overfitting, especially in a complex network with many parameters.\n",
    "\n",
    "    - **Transfer Learning**: Utilizing a pre-trained model on a large dataset and fine-tuning it on your specific dataset can significantly boost the performance. Models like ResNet, VGG, or MobileNet have already learned useful features from large-scale datasets and can be fine-tuned for your specific task.\n",
    "\n",
    "    - **Evaluation Metrics**: Ensure you are using appropriate evaluation metrics for your problem. Sometimes accuracy might not be the best metric, especially in imbalanced datasets. Other metrics like precision, recall, or F1-score might provide better insights.\n",
    "\n",
    "    - **Learning Schedule**: Implementing learning rate schedules or learning rate warm-up can help in stabilizing the training and achieving better performance.\n",
    "\n",
    "    - **Early Stopping**: Implementing early stopping based on validation loss can prevent overfitting and save training time.\n",
    "    \"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f458b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfceab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize data\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bb94e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    horizontal_flip=True,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1\n",
    ")\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c7d622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e988eafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ce4789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = model.fit(datagen.flow(x_train, y_train, batch_size=64),\n",
    "                    epochs=50, \n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9e28f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433dc9d0",
   "metadata": {},
   "source": [
    "#  In this updated snippet:\n",
    "\n",
    "Data normalization and augmentation have been added.\n",
    "The model architecture has been expanded with additional layers, batch normalization, and dropout for regularization.\n",
    "Early stopping is implemented to prevent overfitting.\n",
    "The training is done for more epochs, but with early stopping to halt training if the model is no longer improving on the validation data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
